{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 15:02:48 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 386kB [00:00, 42.3MB/s]\n",
      "2024-09-05 15:02:48 INFO: Downloaded file to /home/xilini/stanza_resources/resources.json\n",
      "2024-09-05 15:02:49 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2024-09-05 15:02:49 INFO: Using device: cpu\n",
      "2024-09-05 15:02:49 INFO: Loading: tokenize\n",
      "2024-09-05 15:02:49 INFO: Loading: mwt\n",
      "2024-09-05 15:02:49 INFO: Loading: pos\n",
      "2024-09-05 15:02:49 INFO: Loading: lemma\n",
      "2024-09-05 15:02:49 INFO: Loading: depparse\n",
      "2024-09-05 15:02:49 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "from collections import defaultdict\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize, mwt, pos, lemma, depparse', use_gpu=False)\n",
    "from merge_intervals import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepGraph:\n",
    "    def __init__(self, text):\n",
    "        self.graph = {}  #{sentid: graph} #defaultdict(dict)\n",
    "        self.paths = {}  #{sentid: list}\n",
    "        self.text = text\n",
    "        self.doc = nlp(text)\n",
    "        self.word_dict = {}\n",
    "        self.pos_dict = {}\n",
    "        self.char_span_dict = {}\n",
    "        self.mentions = []\n",
    "        self.buildgraph()\n",
    "        self.merger = Merger()\n",
    "    \n",
    "    def addEdge(self, s_id, u, v, rel):\n",
    "        '''adds edge u->v with relation rel'''\n",
    "        if(u in self.graph[s_id].keys()):\n",
    "            self.graph[s_id][u][v] = (rel)\n",
    "        else:\n",
    "            self.graph[s_id][u] = {} \n",
    "            self.graph[s_id][u][v] = (rel)\n",
    "            \n",
    "    def addinfo(self, s, u, word, pos, start, end):\n",
    "        self.word_dict[(s, u)] = word\n",
    "        self.pos_dict[(s, u)] = pos\n",
    "        self.char_span_dict[(s, u)] = [start, end]\n",
    "        \n",
    "    def buildgraph(self):\n",
    "        for s_id, sentence in enumerate(self.doc.sentences):\n",
    "            self.graph[s_id] = defaultdict(dict)\n",
    "            for word in sentence.words:\n",
    "                parent_index = word.head\n",
    "                word_index = word.id\n",
    "                relation = word.deprel\n",
    "                self.addEdge(s_id, parent_index, word_index, relation)\n",
    "                self.addinfo(s_id, word_index, word.text, word.pos, word.start_char, word.end_char)\n",
    "            self.find_paths(s_id)\n",
    "        \n",
    "    def find_paths(self, s_id):\n",
    "        self.paths[s_id] = []\n",
    "        self.depthFirst(s_id, 0, -1, [])\n",
    "        list_of_paths = self.paths[s_id]\n",
    "        self.paths[s_id] = {}\n",
    "        for p in list_of_paths:\n",
    "            self.paths[s_id][p[-1]]  = p\n",
    "        \n",
    "    def depthFirst(self, s_id, currentVertex, previousVertex, visited):\n",
    "        visited.append(currentVertex)\n",
    "        for neighbour in self.graph[s_id][currentVertex]:\n",
    "            if neighbour not in visited:\n",
    "                self.depthFirst(s_id, neighbour, currentVertex, visited.copy())\n",
    "        self.paths[s_id].append(visited)\n",
    "        \n",
    "    def noun_phrase(self, sent_id, currentVertex, visited, whitelist=None):\n",
    "        conj_allowed = True\n",
    "        \n",
    "        if whitelist==None:\n",
    "            whitelist = ['compound', 'flat', 'fixed', 'det', 'amod', 'nummod', 'nmod:poss', 'nmod']\n",
    "        else:\n",
    "            if 'conj' not in whitelist:\n",
    "                conj_allowed = False\n",
    "\n",
    "        visited.append(currentVertex)\n",
    "        \n",
    "        if len(self.graph[sent_id][currentVertex])==0:\n",
    "            start_span = self.char_span_dict[(sent_id, currentVertex)][0]\n",
    "            end_span = self.char_span_dict[(sent_id, currentVertex)][1]\n",
    "            return [start_span, end_span]\n",
    "        \n",
    "        start = self.char_span_dict[(sent_id, currentVertex)][0]\n",
    "        end = self.char_span_dict[(sent_id, currentVertex)][1]\n",
    "        \n",
    "        for child in self.graph[sent_id][currentVertex].keys():\n",
    "            if child not in visited:\n",
    "                relation = self.graph[sent_id][currentVertex][child]\n",
    "                if conj_allowed and relation=='conj' and self.pos_dict[(sent_id, child)] in ['PRON', 'NOUN', 'PROPN', 'NUM']:\n",
    "                    [start_new, end_new] = self.noun_phrase(sent_id, child, visited.copy())\n",
    "                    start = min(start_new, start)\n",
    "                    end = max(end_new, end)\n",
    "                elif relation!='conj' and relation in whitelist:\n",
    "                    [start_new, end_new] = self.noun_phrase(sent_id, child, visited.copy())\n",
    "                    start = min(start_new, start)\n",
    "                    end = max(end_new, end)\n",
    "        \n",
    "        return [start, end]\n",
    "    \n",
    "    def get_head_word_char_span(self, mention_spans):\n",
    "        '''mention_spans: [start:end], character spans'''\n",
    "        min_path_len = 1e10 #inf\n",
    "        \n",
    "        lca_span = mention_spans\n",
    "        lca_text = self.text[mention_spans[0]:mention_spans[1]] #self.word_dict[mention_spans[-1]-1]\n",
    "        \n",
    "        for s_id, sent in enumerate(self.doc.sentences):\n",
    "            for word in sent.words:\n",
    "                if word.start_char>=mention_spans[1] or word.end_char<=mention_spans[0]:\n",
    "                    continue\n",
    "                path = self.paths[s_id][word.id]\n",
    "                if len(path)<min_path_len:\n",
    "                    min_path_len = len(path)\n",
    "                    lca_span = [word.start_char, word.end_char]\n",
    "                    lca_text = word.text\n",
    "        return lca_text, lca_span\n",
    "    \n",
    "    def deduplicate(self, mentions, mentions_spans):\n",
    "        mention_heads = {}\n",
    "        for m, s in zip(mentions, mentions_spans):\n",
    "            head_text, head_span = self.get_head_word_char_span(s)\n",
    "            if tuple(head_span) in mention_heads.keys():\n",
    "                mention_heads[tuple(head_span)].append((m, s))\n",
    "            else:\n",
    "                mention_heads[tuple(head_span)] = [(m, s)]\n",
    "        \n",
    "        mentions = []\n",
    "        mentions_spans = []\n",
    "        for v in mention_heads.values():\n",
    "            max_text = ''\n",
    "            max_span = ''\n",
    "            max_len = 0\n",
    "            \n",
    "            for vi in v:\n",
    "                vi_text = vi[0]\n",
    "                vi_span = vi[1]\n",
    "                if len(vi_text)>max_len:\n",
    "                    max_text = vi_text\n",
    "                    max_span = vi_span\n",
    "                    \n",
    "            mentions.append(max_text)\n",
    "            mentions_spans.append(max_span)\n",
    "        return mentions\n",
    "    \n",
    "    def format_str(self, text):\n",
    "        '''1. remove spaces before punctuation marks [,.?!%.-]\n",
    "           2. remove space after -\n",
    "        '''\n",
    "        text = re.sub(r'\\s([-,?.!%\\\"](?:\\s|$))', r'\\1', text)\n",
    "        text = re.sub(r'([-](?:|$))\\s', r'\\1', text).strip()\n",
    "        text = re.sub(r'-LRB-', r'( ', text).strip()\n",
    "        text = re.sub(r'-RRB-', r') ', text).strip()\n",
    "        text = re.sub(r'-LCB-', r'{ ', text).strip()\n",
    "        text = re.sub(r'-RCB-', r'} ', text).strip()\n",
    "        text = re.sub(r'-LSB-', r'[ ', text).strip()\n",
    "        text = re.sub(r'-RSB-', r'] ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def find_mentions(self):\n",
    "        mentions_dict = {}\n",
    "        for s_id, sentence in enumerate(self.doc.sentences):\n",
    "            for word in sentence.words:\n",
    "                if word.pos not in ['PRON', 'NOUN', 'PROPN', 'NUM']:\n",
    "                    continue\n",
    "                mention_char_span = self.noun_phrase(s_id, word.id, [])\n",
    "                if(len(mention_char_span)!=0):\n",
    "                    mentions_dict[(mention_char_span[0], mention_char_span[1])] = self.text[mention_char_span[0]:mention_char_span[1]]\n",
    "        \n",
    "        #merge overlapping spans\n",
    "        mentions_keys = self.merger.merge(list(mentions_dict.keys())) \n",
    "        mentions = []\n",
    "        mentions_spans = []\n",
    "        for k in mentions_keys:\n",
    "            if (k[0], k[1]) in mentions_dict.keys():\n",
    "                mentions.append(mentions_dict[(k[0], k[1])])\n",
    "                mentions_spans.append((k[0], k[1]))\n",
    "            else:\n",
    "                mentions.append(self.text[k[0]:k[1]-1])\n",
    "                mentions_spans.append((k[0], k[1]))\n",
    "            \n",
    "  \n",
    "        #head word based de-duplication\n",
    "        mentions = self.deduplicate(mentions, mentions_spans)\n",
    "\n",
    "\n",
    " \n",
    "        # Rule 1: add the token heading a cordinated phrase and its children to the list of mentions\n",
    "        # example, if text = '''Bob, John, and Mary saw him.''', add \"Bob\", \"John\" and \"Mary\" to list of mentions\n",
    "        conjunct_whitelist = ['compound', 'flat', 'fixed', 'amod', 'nummod', 'nmod:poss', 'nmod']\n",
    "        \n",
    "        for s_id, sentence in enumerate(self.doc.sentences):\n",
    "            for word in sentence.words:\n",
    "                if (word.pos in ['NOUN', 'PROPN', 'PRON']):\n",
    "                    for child in self.graph[s_id][word.id]:\n",
    "                        relation = self.graph[s_id][word.id][child]\n",
    "                        if relation=='conj' and self.pos_dict[(s_id, word.id)] in ['NOUN', 'PROPN', 'PRON']:  \n",
    "                            #parent of conj\n",
    "                            \n",
    "                            mention_char_span = self.noun_phrase(s_id, word.id, [], conjunct_whitelist)\n",
    "                            if(len(mention_char_span)!=0):\n",
    "                                conjunct_text = self.text[mention_char_span[0]:mention_char_span[1]]\n",
    "                                conjunct_span = (mention_char_span[0], mention_char_span[1])\n",
    "                                #print('word_text: ', word.text)\n",
    "                                #print('conjunct_text: ', conjunct_text)\n",
    "                            else:\n",
    "                                conjunct_text = word.text\n",
    "                                conjunct_span = (word.start_char, word.end_char)\n",
    "                            if conjunct_span not in mentions:\n",
    "                                mentions.append(conjunct_text)\n",
    "                                mentions_spans.append(conjunct_span)\n",
    "                            \n",
    "                            #child of conj\n",
    "                            if self.pos_dict[(s_id, child)] in ['NOUN', 'PROPN', 'PRON']:\n",
    "                                child_mention_char_span = self.noun_phrase(s_id, child, [], conjunct_whitelist)\n",
    "                                if(len(child_mention_char_span)!=0):\n",
    "                                    conjunct_child_text = self.text[child_mention_char_span[0]:child_mention_char_span[1]]\n",
    "                                    conjunct_child_span = (child_mention_char_span[0], child_mention_char_span[1])\n",
    "                                    #print('child_word_text: ', self.word_dict[(s_id, child)])\n",
    "                                    #print('child_conjunct_text: ', conjunct_child_text)\n",
    "                                else:\n",
    "                                    conjunct_child_text = self.word_dict[(s_id, child)]\n",
    "                                    conjunct_child_span = (self.char_span_dict[(s_id, child)][0], self.char_span_dict[(s_id, child)][1])\n",
    "                                if conjunct_child_span not in mentions:\n",
    "                                    mentions.append(conjunct_child_text)\n",
    "                                    mentions_spans.append(conjunct_child_span)\n",
    "                                    \n",
    "        #Rule 2: add child of nmod:poss relation, to include possesive pronouns, names as mentions \n",
    "        for s_id, sentence in enumerate(self.doc.sentences):\n",
    "            for word in sentence.words:\n",
    "                if (word.pos in ['PRON', 'NOUN', 'PROPN'] and word.deprel in [\"nmod:poss\", \"nmod\"]):\n",
    "                    mention_char_span = self.noun_phrase(s_id, word.id, [])\n",
    "                    if(len(mention_char_span)!=0):\n",
    "                        mentions.append(self.text[mention_char_span[0]:mention_char_span[1]])\n",
    "                        mentions_spans.append((mention_char_span[0], mention_char_span[1]))\n",
    "                    else:\n",
    "                        mentions.append(word.text)\n",
    "                        mentions_spans.append((word.start_char, word.end_char))\n",
    "                        \n",
    "   \n",
    "        #Rule 3: parent--compound-->child, if parent is NN and child is proper noun, include child as mention\n",
    "        for s_id, sentence in enumerate(self.doc.sentences):\n",
    "            for word in sentence.words:\n",
    "                parent = sentence.words[word.head-1]\n",
    "                if (word.pos in ['PROPN'] and parent.pos in ['NOUN'] and word.deprel in [\"compound\", \"flat\", \"fixed\"]):\n",
    "                    mention_char_span = self.noun_phrase(s_id, word.id, [])\n",
    "                    if(len(mention_char_span)!=0):\n",
    "                        mentions.append(self.text[mention_char_span[0]:mention_char_span[1]])\n",
    "                        mentions_spans.append((mention_char_span[0], mention_char_span[1]))\n",
    "                    else:\n",
    "                        mentions.append(word.text)\n",
    "                        mentions_spans.append((word.start_char, word.end_char))\n",
    "\n",
    "        return mentions, mentions_spans\n",
    "    \n",
    "    \n",
    "    def copular_mentions(self):\n",
    "        copular_heads = []\n",
    "        for sent_id, sent in enumerate(self.doc.sentences):\n",
    "            for word in sent.words:\n",
    "                word_id = word.id\n",
    "                for child in self.graph[sent_id][word_id].keys():\n",
    "                    relation = self.graph[sent_id][word_id][child]\n",
    "                    if relation=='cop' and word.pos in ['NOUN', 'ADJ', 'PROPN', 'PRON']:\n",
    "                        copular_heads.append((sent_id, word))\n",
    "\n",
    "        copula_mention_pairs = []               \n",
    "        for (sent_id, word) in copular_heads:\n",
    "            sent = self.doc.sentences[sent_id]\n",
    "            #exlude PP\n",
    "            if 'case' in self.graph[sent_id][word.id].values():\n",
    "                continue\n",
    "                \n",
    "            #look for subject  \n",
    "            for child in self.graph[sent_id][word.id].keys():\n",
    "                relation = self.graph[sent_id][word.id][child]\n",
    "                child_pos = self.pos_dict[(sent_id, child)]\n",
    "                if relation in ['nsubj'] and child_pos in ['PROPN', 'NOUN', 'PRON']:\n",
    "                    word_span = (word.start_char, word.end_char)\n",
    "                    child_char_span = self.char_span_dict[(sent_id, child)]\n",
    "                    child_span = (child_char_span[0], child_char_span[1])\n",
    "                    copula_mention_pairs.append([word_span, child_span])\n",
    "        return copula_mention_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 20/20 [08:00<00:00, 24.02s/it]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import tqdm\n",
    "\n",
    "file_loc = '../../multimodal_coref/data/nikolai_test_out/randomsample_20/original/'\n",
    "out_loc = '../../multimodal_coref/data/nikolai_test_out/randomsample_20/ez-mentions/'\n",
    "original_samples = glob.glob(file_loc + '*')\n",
    "ez_mentions = {}\n",
    "\n",
    "for file in tqdm.tqdm(original_samples):\n",
    "    sample_id = file.split('/')[-1]\n",
    "    with open(file, 'r') as f:\n",
    "        text = f.read().splitlines()\n",
    "    text = ' '.join(text).strip()\n",
    "    text = f\"'''{text}'''\"\n",
    "    #print(text)\n",
    "\n",
    "    g = DepGraph(text)\n",
    "    mention_text, mention_char_spans = g.find_mentions()\n",
    "    #print('mention', 'char_span')\n",
    "    with open(out_loc + sample_id, 'w') as f:\n",
    "        for t, s in zip(mention_text, mention_char_spans):\n",
    "            #print(t, s)\n",
    "            f.write(t + '\\t' + str(s) + '\\n')\n",
    "    \n",
    "    #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mention char_span\n",
      "Mao Tse-tung's China (5, 25)\n",
      "1949 (35, 39)\n",
      "a partnership between the communists and a number of smaller, non-communist parties (45, 128)\n",
      "communists (71, 81)\n",
      "number of smaller, non-communist parties (88, 128)\n",
      "Mao Tse-tung (5, 17)\n",
      "the communists and a number of smaller, non-communist parties (67, 128)\n",
      "smaller, non-communist parties (98, 128)\n"
     ]
    }
   ],
   "source": [
    "text = '''Even Mao Tse-tung's China began in 1949 with a partnership between the communists and a number of smaller, non-communist parties.'''\n",
    "g = DepGraph(text)\n",
    "mention_text, mention_char_spans = g.find_mentions()\n",
    "print('mention', 'char_span')\n",
    "for t, s in zip(mention_text, mention_char_spans):\n",
    "    print(t, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to front-end format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert spans to format acceptable by the front end \n",
    "\n",
    "def construct_tok_dict(word, left_offset, right_offset, pos, candidate=False, target=False):\n",
    "    return {'word':word,\\\n",
    "            'left_offset':left_offset,\\\n",
    "            'right_offset':right_offset,\\\n",
    "            'candidate':candidate,\\\n",
    "            'target':target,\\\n",
    "            'pos':pos}\n",
    "\n",
    "def find_sublist_idx(sentence_tokens, mention_tokens):\n",
    "    mention_len = len(mention_tokens)\n",
    "    sentence_len = len(sentence_tokens)\n",
    "    mention_occurrences = []\n",
    "    for i in range(sentence_len):\n",
    "        if sentence_tokens[i: min(i+mention_len, sentence_len)]==mention_tokens:\n",
    "            mention_occurrences.append(i)\n",
    "    return mention_occurrences\n",
    "\n",
    "def flatten_list(regular_list):\n",
    "    return [item for sublist in regular_list for item in sublist]\n",
    "\n",
    "def length_of_span(span):\n",
    "    [s, e] = span\n",
    "    key = e-s\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key is span, text is value in system_mentions_string_inverse\n",
    "# key is text, span is value in system_mentions_string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_for_frontend(text):\n",
    "    #find mentions with character-level spans\n",
    "    g = DepGraph(text)\n",
    "    mention_text, mention_char_spans = g.find_mentions()\n",
    "\n",
    "        \n",
    "    from spacy.lang.en import English\n",
    "    nlp = English()\n",
    "    simple_tokenizer = nlp.tokenizer\n",
    "    \n",
    "    sentence_tokens = []\n",
    "    for token in simple_tokenizer(text):\n",
    "        sentence_tokens.append(token.text)\n",
    "    \n",
    "    #represent the sentence as a list of tokens\n",
    "    new_sent = []\n",
    "    for word in sentence_tokens:\n",
    "        new_sent.append(construct_tok_dict(word, 0, 1, 'pos', candidate=False, target=False))\n",
    "    \n",
    "\n",
    "    # map to token-level spans\n",
    "    for text, span in zip(mention_text, mention_char_spans):\n",
    "        mention_tokens = []\n",
    "        for token in simple_tokenizer(text):\n",
    "            mention_tokens.append(token.text)\n",
    "\n",
    "        new_chunk = []\n",
    "\n",
    "        left_offset = 0\n",
    "        right_offset = len(mention_tokens)\n",
    "        for m_tok in mention_tokens:\n",
    "            new_chunk.append(construct_tok_dict(m_tok, left_offset,\\\n",
    "                                                right_offset, 'pos',\\\n",
    "                                                candidate=True, target=True))\n",
    "            left_offset-=1\n",
    "            right_offset-=1\n",
    "\n",
    "        start_idxs = find_sublist_idx(sentence_tokens, mention_tokens)\n",
    "        for occurrence in start_idxs:\n",
    "            new_sent[occurrence:occurrence+len(mention_tokens)] = new_chunk \n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['''Even Mao Tse-tung's China began in 1949 with a partnership between the communists and a number of smaller, non-communist parties.''',\\\n",
    "            '''Some U.S. allies are complaining that President Bush is pushing conventional-arms talks too quickly, creating a risk that negotiators will make errors that could affect the security of Western Europe for years.'''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 'test'\n",
    "frontend_doc = []\n",
    "for idx, sent in enumerate(sentences):\n",
    "    new_sent = get_sentences_for_frontend(sent)\n",
    "    frontend_doc.append({'doc_id': doc_id, \\\n",
    "                         'sent_id': idx, \\\n",
    "                         'targets': [], \\\n",
    "                         'tokens':new_sent})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coref",
   "language": "python",
   "name": "coref"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
